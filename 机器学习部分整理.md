



### 1特征构建

对收集到的特征根据特征层次分为5 类特征：

原子层面

**Atomic number factor**(13 properties)  

**Electro-chemical factor** (13 properties)  

**Group number factor** (6 properties)



**Size factor** (13 properties)

> 还有一个density不知道怎么分
>

特征分类的事情先往后放一放。



采用加权平均和方差平均的计算方式将成分特征转化为化合物的物理特征，建立 经验模型。加权平均和方差平均计算方式为：
![Snipaste_2022-03-13_15-45-35](D:\路径不动的文件\图片\Snipaste_2022-03-13_15-45-35.png)

![Snipaste_2022-03-13_15-46-02](D:\路径不动的文件\图片\Snipaste_2022-03-13_15-46-02.png)
其中 fn ，pn代表化合物原子百分占比，X ，C 代表化合物各个元素组成。

### 2 皮尔逊相关性筛选

![Snipaste_2022-03-13_15-47-55](D:\路径不动的文件\图片\Snipaste_2022-03-13_15-47-55.png)

第一步是相关性筛选，我们计算了任何两个合金因素之间的Pearson线性相关系数r。
在r值>0.95的情况下，保留与目标特征相关性大的那一个特征。

> 付老师：
>
> 我们将这两个合金因素作为输入，分别建立模型，并剔除合金因素误差较大的合金因素，并将其作为模型的输入。
> 模型误差较大的合金因素淘汰。



### 3 模型选择与递归消除

经过皮尔逊相关系数特征筛选后，剩余的特征数量仍比较多

我们选择了DecisionTreeClassifier）、GradientBoostingClassifier、XGB、KNeighborsClassifier、支持向量机径向基核函数（svc）、随机森林（rfc）8 种不同的学习算法模型对数据进行拟合，建立训练模型。

XGBClassifier

|      | model                                       | accuracy |
| ---- | ------------------------------------------- | -------- |
| 1    | XGBClassifier                               | 0.9339   |
| 2    | RandomForestClassifier                      | 0.9347   |
| 3    | KNeighborsClassifier                        | 0.7085   |
| 4    | GradientBoostingClassifier                  | 0.9307   |
| 5    | DecisionTreeClassifier                      | 0.9012   |
| 6    | svm.SVC(kernel=**'rbf'**, gamma=**'auto'**) | 0.5191   |

*[0.9339,* **0.9347**, 0.7085, 0.9307, 0.9012, 0.5191]



选择**随机森林**模型进行建模。

**递归消除。**（这个图也可以不要折线段）
对于经过相关性筛选后的剩余合金因素，我们依次从n个合金因子中选择一个，并将剩余的n-1个合金因子作为输入，建立rf模型，然后通过10次的10-fold计算出 
通过10次的10倍交叉验证，计算模型误差。

将最小的准确率合金因子相对应的 特征剔除
然后将模型误差最小的合金因子重新迁移，剩下n-1个合金因子。我们反复进行消除，直到最小的模型误差发生变化。

> 直到最小的模型误差从一个向下的到上升的趋势。



### 穷举

第三步是详尽的筛选。（穷举图不要折线段了）

> 付老师：通过建立一系列的SVR模型，使用递归消除步骤中剩余的合金因素的所有详尽的组合递归消除步骤中剩余的合金因素的所有详尽组合作为输入。我们最终确定了最能影响合金特性的关键合金因素。
> 通过比较模型误差，我们最终确定了对合金性能影响最大的关键合金因素。参见 
> 我们已发表的工作[16]，了解具体的筛选方法。





穷举图    显示，随着特征数量的增加，ML模型的误差最初会下降。ML模型的误差最初减少，表明模型的改进的模型。然而，随着更多特征的加入，模型误差趋于增加或趋于平缓。随着更多的特征被添加为输入，模型误差基本保持不变或略微增加。在数据量相同的情况下，增加更多的特征数据的情况下，增加更多的特征并不一定能改善模型，而是表明了过度优化。利用 "奥卡姆剃刀 "原则，我们选择了简单和可解释性的一方，我们采用 "奥卡姆剃刀 "原则，从简单和可解释的角度出发，使用足够准确所需的最少的特征。使用更多的特征会使模型变得更加复杂，这有可能导致过度学习，摄取不太重要的特征。**我们选择特征子集个数为三的特征集合进行下一步选择。**



我们计算了这些特征集合到我们的预测空间的距离，（标准欧氏距离）（写不写上），并通过下面这个方法进行模型最终的选择（我建议选二）（结合前沿面，建议from师兄师姐）

1. 

> 第一步：
>
> 取准确率前10%。
>
> 第二步：
>
> 取距离10%
>
> 第三步：
>
> 前沿面最优

-----

2. 

> 第一步：
>
> 取准确率前10%。
>
> 第二步：
>
> 取距离最近的

-----

3. 

> 第一步：
>
> 画前沿面
>
> 取准确率前10%。
>
> 第二步：
>
> 取距离最近



![Snipaste_2022-03-14_09-17-05](D:\路径不动的文件\图片\Snipaste_2022-03-14_09-17-05.png)

### 预测空间

Co 50-100 

A - 1-50

B 1-50

| **ternary** |
| ----------- |
| **CoNiTi**  |
| **CoNiNb**  |
| **CoNiTa**  |
| **CoAlCr**  |
| **CoTiNb**  |
| **CoTaNb**  |
| **CoAlNb**  |
| **CoMoNb**  |
| **CoCrNb**  |
| **CoTaCr**  |
| **CoCrV**   |
| **CoAlTi**  |
| **CoWTa**   |
| **CoWNb**   |
| **CoTaMo**  |
| **CoNiV**   |

| 0    | δ lowest Energy of the atomic  orbitals |
| ---- | --------------------------------------- |
| 1    | δ d Orbital level                       |
| 3    | Valence                                 |

### 预测结果

各个三元体系的预测结果按照其预测存在γ' 个数进行了排序：

 CoNiTa >  CoNiTi >  CoAlTi >  CoTiNb > CoWTa >CoTaNb >CoTaMo > CoNiV  ≥ 50

预测结果中个数小于50的被认为是不被考虑的
